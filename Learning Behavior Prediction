# -*- coding: utf-8 -*-
"""EDM HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_nHrOq9EUl1xDFh0s8N1OYhjeIttrji6

**Q1 build a decision tree on the entire data set. What is kappa?**
"""

import numpy as np
import pandas as pd

df = pd.read_csv('ba1-in.csv')

from sklearn import tree
from sklearn.metrics import cohen_kappa_score

data1 = pd.get_dummies(df, columns=['SCHOOL', 'Class', 'CODER', 'Activity'])

data1

y = data1['ONTASK']

X = data1.drop(['ONTASK'], axis=1)

clf = tree.DecisionTreeClassifier(min_samples_split=10)
clf = clf.fit(X, y)

y_predict = clf.predict(X)

cohen_kappa_score( y_predict, y )

"""**Q2 what is kappa if you build model excluding student?**"""

x2 = data1.drop(['ONTASK','STUDENTID'], axis=1)

clf = tree.DecisionTreeClassifier(min_samples_split=10)
clf = clf.fit(x2, y)

y_predict = clf.predict(x2)

cohen_kappa_score( y_predict, y )

"""**Q 4 what is the non-cross-validated kappa, if you build the decision tree model (use the same operator), excluding student and the variables from question 3?**"""

data2 = pd.get_dummies(df, columns=['Activity'])

x3 = data2.drop(['ONTASK','SCHOOL', 'Class', 'CODER','UNIQUEID','STUDENTID'], axis=1)

y = data2['ONTASK']

clf = tree.DecisionTreeClassifier(min_samples_split=10)
clf = clf.fit(x3, y)

y_predict = clf.predict(x3)

cohen_kappa_score( y_predict, y )

"""**Q5 What is the non-cross-validated kappa for the same set of variables you used for Question 4 if you use Naive Bayes instead of CART?**"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(x3, y)

predictions = gnb.predict(x3)

cohen_kappa_score(predictions, y)

"""**Q6 What is the non-cross validated kappa for the same set of variables you used for Question 4 if you use extreme gradient boosting instead of CART?**"""

from xgboost import XGBClassifier

XGB = XGBClassifier(learning_rate=0.5, n_estimators=200, random_state=5)
XGB.fit(x3, y)

predictions = XGB.predict(x3)

cohen_kappa_score(predictions, y)

"""**Q 7 What is the kappa, if you delete School, Class, Coder, UNIQUEID, and STUDENTID, use CART, and conduct 10-fold student-level batch cross-validation using the basic decision tree classifier?**"""

data2 = pd.get_dummies(df, columns=['Activity'])

x3 = data2.drop(['ONTASK', 'SCHOOL', 'Class', 'CODER','UNIQUEID','STUDENTID'], axis=1)
y = data2['ONTASK']

from sklearn.model_selection import GroupKFold

group_dict = {}
groups = np.array([])

for index, row in data1.iterrows():
    student_id = row['STUDENTID']
    if student_id not in group_dict:
        group_dict[student_id] = index
    groups = np.append(groups, group_dict[student_id])

gkf = GroupKFold(n_splits=10)

kappa_values = list()

for train_index, test_index in gkf.split(x3, y, groups=groups):
    x3_train = x3.iloc[train_index]
    x3_test = x3.iloc[test_index]
    y_train = y[train_index]
    y_test = y[test_index]
    clf = tree.DecisionTreeClassifier(min_samples_split=10)
    clf.fit(x3_train, y_train)
    predictions = clf.predict(x3_test)
    kappa = cohen_kappa_score(y_test, predictions)
    kappa_values.append(kappa)
kappa_values

print('Mean:',np.mean(kappa_values))

"""**Q8 What is the kappa, for the same set of variables you used for question 4, if you use Naive Bayes, and conduct 10-fold student-level batch cross-validation?**"""

data2 = pd.get_dummies(df, columns=['Activity'])
x3 = data2.drop(['ONTASK', 'SCHOOL', 'Class', 'CODER','UNIQUEID','STUDENTID'], axis=1)
y = data2['ONTASK']

group_dict = {}
groups = np.array([])

for index, row in data1.iterrows():
    student_id = row['STUDENTID']
    if student_id not in group_dict:
        group_dict[student_id] = index
    groups = np.append(groups, group_dict[student_id])

gkf = GroupKFold(n_splits=10)

kappa_values = list()

for train_index, test_index in gkf.split(x3, y, groups=groups):
    x3_train = x3.iloc[train_index]
    x3_test = x3.iloc[test_index]
    y_train = y[train_index]
    y_test = y[test_index]
    gnb = GaussianNB()
    gnb.fit(x3_train, y_train.ravel())
    predictions = gnb.predict(x3_test)
    kappa = cohen_kappa_score(y_test, predictions)
    kappa_values.append(kappa)
kappa_values

print('Mean:',np.mean(kappa_values))

"""**Q 9 What is the kappa, for the same set of variables you used for question 4, if you use Extreme Gradient Boosting, and conduct 10-fold student-level batch cross-validation?**"""

data2 = pd.get_dummies(df, columns=['Activity'])
x3 = data2.drop(['ONTASK', 'SCHOOL', 'Class', 'CODER','UNIQUEID','STUDENTID'], axis=1)
y = data2['ONTASK']

group_dict = {}
groups = np.array([])

for index, row in data1.iterrows():
    student_id = row['STUDENTID']
    if student_id not in group_dict:
        group_dict[student_id] = index
    groups = np.append(groups, group_dict[student_id])

gkf = GroupKFold(n_splits=10)

kappa_values = list()

for train_index, test_index in gkf.split(x3, y, groups=groups):
    x3_train = x3.iloc[train_index]
    x3_test = x3.iloc[test_index]
    y_train = y[train_index]
    y_test = y[test_index]
    xgb = XGBClassifier(learning_rate=0.5, n_estimators=200, random_state=5)
    xgb.fit(x3_train, y_train)
    predictions = xgb.predict(x3_test)
    kappa = cohen_kappa_score(y_test, predictions)
    kappa_values.append(kappa)
kappa_values

print('Mean:',np.mean(kappa_values))
